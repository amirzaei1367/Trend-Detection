{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "inputHidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at 'In [18]'.</span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at 'In [18]'.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 0.018768,
     "end_time": "2019-05-14T20:50:24.629422",
     "exception": false,
     "start_time": "2019-05-14T20:50:24.610654",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_cores = 27\n",
    "mem = 650000\n",
    "config = \"configs.config_Reddit\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T20:35:33.781563Z",
     "start_time": "2019-05-14T20:35:33.778425Z"
    },
    "code_folding": [
     0
    ],
    "papermill": {
     "duration": 0.016348,
     "end_time": "2019-05-14T20:50:24.656897",
     "exception": false,
     "start_time": "2019-05-14T20:50:24.640549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### The steps of preprocessing:\n",
    "\n",
    "# 1- cleaning\n",
    "# 2- Unigram\n",
    "#     2-1 - English\n",
    "#     2-2 - Stop Words\n",
    "#     2-3 - Stemming\n",
    "# 3- Bigram\n",
    "# 4- Tokenization\n",
    "# 6- Lemmatization\n",
    "# 7- Thresholding: \n",
    "#     7-1 min < word < % of corupus\n",
    "#     7-2 min_words_per_docs\n",
    "# 8- LDA\n",
    "\n",
    "### The steps of preprocessing for tokens:\n",
    "\n",
    "# 1- cleaning\n",
    "# 2- Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T20:35:33.788682Z",
     "start_time": "2019-05-14T20:35:33.785232Z"
    },
    "code_folding": [
     0
    ],
    "papermill": {
     "duration": 0.016225,
     "end_time": "2019-05-14T20:50:24.683828",
     "exception": false,
     "start_time": "2019-05-14T20:50:24.667603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "if 'config' not in locals():\n",
    "#     config = 'configs.config_SE_test_local'\n",
    "    config = 'configs.config_Reddit_test_local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T20:35:40.448266Z",
     "start_time": "2019-05-14T20:35:33.816669Z"
    },
    "code_folding": [
     0
    ],
    "papermill": {
     "duration": 9.213225,
     "end_time": "2019-05-14T20:50:33.907772",
     "exception": false,
     "start_time": "2019-05-14T20:50:24.694547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config file is set to configs.config_Reddit\n",
      "num_cores is set to 27\n",
      "mem is set to 650000\n"
     ]
    }
   ],
   "source": [
    "### initialization ###\n",
    "\n",
    "from _imports import *\n",
    "# from _utils import *\n",
    "\n",
    "print('config file is set to {}'.format(config))\n",
    "\n",
    "util=importlib.import_module('_utils')\n",
    "importlib.reload(util)\n",
    "\n",
    "c=importlib.import_module(config)\n",
    "importlib.reload(c)\n",
    "\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "# param_setup(sys.argv[1:], c)\n",
    "util.param_setup_ipython(globals(), c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T20:35:40.535642Z",
     "start_time": "2019-05-14T20:35:40.451520Z"
    },
    "code_folding": [
     0,
     5
    ],
    "papermill": {
     "duration": 109.454488,
     "end_time": "2019-05-14T20:52:23.373873",
     "exception": false,
     "start_time": "2019-05-14T20:50:33.919385",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all_RS_2014-01_all.csv',\n",
      " 'all_RS_2014-02_all.csv',\n",
      " 'all_RS_2014-03_all.csv',\n",
      " 'all_RS_2014-04_all.csv',\n",
      " 'all_RS_2014-05_all.csv',\n",
      " 'all_RS_2014-06_all.csv',\n",
      " 'all_RS_2014-07_all.csv',\n",
      " 'all_RS_2014-08_all.csv',\n",
      " 'all_RS_2014-09_all.csv',\n",
      " 'all_RS_2014-10_all.csv',\n",
      " 'all_RS_2014-11_all.csv',\n",
      " 'all_RS_2014-12_all.csv',\n",
      " 'all_RS_2015-01_all.csv',\n",
      " 'all_RS_2015-02_all.csv',\n",
      " 'all_RS_2015-03_all.csv',\n",
      " 'all_RS_2015-04_all.csv',\n",
      " 'all_RS_2015-05_all.csv',\n",
      " 'all_RS_2015-06_all.csv',\n",
      " 'all_RS_2015-07_all.csv',\n",
      " 'all_RS_2015-08_all.csv',\n",
      " 'all_RS_2015-09_all.csv',\n",
      " 'all_RS_2015-10_all.csv',\n",
      " 'all_RS_2015-11_all.csv',\n",
      " 'all_RS_2015-12_all.csv',\n",
      " 'all_RS_2016-01_all.csv',\n",
      " 'all_RS_2016-02_all.csv',\n",
      " 'all_RS_2016-03_all.csv',\n",
      " 'all_RS_2016-04_all.csv',\n",
      " 'all_RS_2016-05_all.csv',\n",
      " 'all_RS_2016-06_all.csv',\n",
      " 'all_RS_2016-07_all.csv',\n",
      " 'all_RS_2016-08_all.csv',\n",
      " 'all_RS_2016-09_all.csv',\n",
      " 'all_RS_2016-10_all.csv',\n",
      " 'all_RS_2016-11_all.csv',\n",
      " 'all_RS_2016-12_all.csv',\n",
      " 'all_RS_2017-01_all.csv',\n",
      " 'all_RS_2017-02_all.csv',\n",
      " 'all_RS_2017-03_all.csv',\n",
      " 'all_RS_2017-04_all.csv',\n",
      " 'all_RS_2017-05_all.csv',\n",
      " 'all_RS_2017-06_all.csv',\n",
      " 'all_RS_2017-07_all.csv',\n",
      " 'all_RS_2017-08_all.csv',\n",
      " 'all_RS_2017-09_all.csv',\n",
      " 'all_RS_2017-10_all.csv',\n",
      " 'all_RS_2017-11_all.csv',\n",
      " 'all_RS_2017-12_all.csv',\n",
      " 'all_RS_2018-01_all.csv',\n",
      " 'all_RS_2018-02_all.csv',\n",
      " 'all_RS_2018-03_all.csv',\n",
      " 'all_RS_2018-04_all.csv',\n",
      " 'all_RS_2018-05_all.csv',\n",
      " 'all_RS_2018-06_all.csv',\n",
      " 'all_RS_2018-07_all.csv',\n",
      " 'all_RS_2018-08_all.csv',\n",
      " 'all_RS_2018-09_all.csv',\n",
      " 'all_RS_2018-10_all.csv',\n",
      " 'all_RS_2018-11_all.csv',\n",
      " 'all_RS_2018-12_all.csv']\n",
      "all_RS_2014-01_all.csv file has {} corrupt rows!\n",
      "all_RS_2014-02_all.csv file has {} corrupt rows!\n",
      "all_RS_2014-03_all.csv file has {} corrupt rows!\n",
      "all_RS_2014-04_all.csv file has {} corrupt rows!\n",
      "all_RS_2014-05_all.csv file has {} corrupt rows!\n",
      "all_RS_2014-06_all.csv file has {} corrupt rows!\n",
      "all_RS_2014-07_all.csv file has {} corrupt rows!\n",
      "all_RS_2014-08_all.csv file has {} corrupt rows!\n",
      "all_RS_2014-09_all.csv file has {} corrupt rows!\n",
      "all_RS_2014-10_all.csv file has {} corrupt rows!\n",
      "all_RS_2014-11_all.csv file has {} corrupt rows!\n",
      "all_RS_2014-12_all.csv file has {} corrupt rows!\n",
      "all_RS_2015-01_all.csv file has {} corrupt rows!\n",
      "all_RS_2015-02_all.csv file has {} corrupt rows!\n",
      "all_RS_2015-03_all.csv file has {} corrupt rows!\n",
      "all_RS_2015-04_all.csv file has {} corrupt rows!\n",
      "all_RS_2015-05_all.csv file has {} corrupt rows!\n",
      "all_RS_2015-06_all.csv file has {} corrupt rows!\n",
      "all_RS_2015-07_all.csv file has {} corrupt rows!\n",
      "all_RS_2015-08_all.csv file has {} corrupt rows!\n",
      "all_RS_2015-09_all.csv file has {} corrupt rows!\n",
      "all_RS_2015-10_all.csv file has {} corrupt rows!\n",
      "all_RS_2015-11_all.csv file has {} corrupt rows!\n",
      "all_RS_2015-12_all.csv file has {} corrupt rows!\n",
      "all_RS_2016-01_all.csv file has {} corrupt rows!\n",
      "all_RS_2016-02_all.csv file has {} corrupt rows!\n",
      "all_RS_2016-03_all.csv file has {} corrupt rows!\n",
      "all_RS_2016-04_all.csv file has {} corrupt rows!\n",
      "all_RS_2016-05_all.csv file has {} corrupt rows!\n",
      "all_RS_2016-06_all.csv file has {} corrupt rows!\n",
      "all_RS_2016-07_all.csv file has {} corrupt rows!\n",
      "all_RS_2016-08_all.csv file has {} corrupt rows!\n",
      "all_RS_2016-09_all.csv file has {} corrupt rows!\n",
      "all_RS_2016-10_all.csv file has {} corrupt rows!\n",
      "all_RS_2016-11_all.csv file has {} corrupt rows!\n",
      "all_RS_2016-12_all.csv file has {} corrupt rows!\n",
      "all_RS_2017-01_all.csv file has {} corrupt rows!\n",
      "all_RS_2017-02_all.csv file has {} corrupt rows!\n",
      "all_RS_2017-03_all.csv file has {} corrupt rows!\n",
      "all_RS_2017-04_all.csv file has {} corrupt rows!\n",
      "all_RS_2017-05_all.csv file has {} corrupt rows!\n",
      "all_RS_2017-06_all.csv file has {} corrupt rows!\n",
      "all_RS_2017-07_all.csv file has {} corrupt rows!\n",
      "all_RS_2017-08_all.csv file has {} corrupt rows!\n",
      "all_RS_2017-09_all.csv file has {} corrupt rows!\n",
      "all_RS_2017-10_all.csv file has {} corrupt rows!\n",
      "all_RS_2017-11_all.csv file has {} corrupt rows!\n",
      "all_RS_2017-12_all.csv file has {} corrupt rows!\n",
      "all_RS_2018-01_all.csv file has {} corrupt rows!\n",
      "all_RS_2018-02_all.csv file has {} corrupt rows!\n",
      "all_RS_2018-03_all.csv file has {} corrupt rows!\n",
      "all_RS_2018-04_all.csv file has {} corrupt rows!\n",
      "all_RS_2018-05_all.csv file has {} corrupt rows!\n",
      "all_RS_2018-06_all.csv file has {} corrupt rows!\n",
      "all_RS_2018-07_all.csv file has {} corrupt rows!\n",
      "all_RS_2018-08_all.csv file has {} corrupt rows!\n",
      "all_RS_2018-09_all.csv file has {} corrupt rows!\n",
      "all_RS_2018-10_all.csv file has {} corrupt rows!\n",
      "all_RS_2018-11_all.csv file has {} corrupt rows!\n",
      "all_RS_2018-12_all.csv file has {} corrupt rows!\n",
      "'Number of rows for submissions = 249873'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>num_crossposts</th>\n",
       "      <th>permalink</th>\n",
       "      <th>score</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>subreddit_type</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nathancrumpton</td>\n",
       "      <td>1390156402</td>\n",
       "      <td>1vlv7n</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/r/dataisbeautiful/comments/1vlv7n/the_top_20_...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dataisbeautiful</td>\n",
       "      <td>t5_2tk95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The top 20 Metro Areas in the United States, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>1390157365</td>\n",
       "      <td>1vlwln</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/r/MachineLearning/comments/1vlwln/in_the_begi...</td>\n",
       "      <td>0</td>\n",
       "      <td>Near the beginning of awareness, the Universal...</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>t5_2r3gv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In the Beginning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wanderingstan</td>\n",
       "      <td>1390160408</td>\n",
       "      <td>1vm12a</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/r/dataisbeautiful/comments/1vm12a/influence_m...</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dataisbeautiful</td>\n",
       "      <td>t5_2tk95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(Influence mapping of) Programming Languages a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>poopballs</td>\n",
       "      <td>1390168955</td>\n",
       "      <td>1vmdrv</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/r/dataisbeautiful/comments/1vmdrv/comparison_...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dataisbeautiful</td>\n",
       "      <td>t5_2tk95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Comparison Chart of different aircraft carrier...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mister_Six</td>\n",
       "      <td>1390169441</td>\n",
       "      <td>1vmei2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/r/dataisbeautiful/comments/1vmei2/welfare_fra...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dataisbeautiful</td>\n",
       "      <td>t5_2tk95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Welfare fraud vs Tax evasion/avoidance estimat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all_RC_2014-01_all_.csv',\n",
      " 'all_RC_2014-02_all_.csv',\n",
      " 'all_RC_2014-03_all_.csv',\n",
      " 'all_RC_2014-04_all_.csv',\n",
      " 'all_RC_2014-05_all_.csv',\n",
      " 'all_RC_2014-06_all_.csv',\n",
      " 'all_RC_2014-07_all_.csv',\n",
      " 'all_RC_2014-08_all_.csv',\n",
      " 'all_RC_2014-09_all_.csv',\n",
      " 'all_RC_2014-10_all_.csv',\n",
      " 'all_RC_2014-11_all_.csv',\n",
      " 'all_RC_2014-12_all_.csv',\n",
      " 'all_RC_2015-01_all_.csv',\n",
      " 'all_RC_2015-02_all_.csv',\n",
      " 'all_RC_2015-03_all_.csv',\n",
      " 'all_RC_2015-04_all_.csv',\n",
      " 'all_RC_2015-05_all_.csv',\n",
      " 'all_RC_2015-06_all_.csv',\n",
      " 'all_RC_2015-07_all_.csv',\n",
      " 'all_RC_2015-08_all_.csv',\n",
      " 'all_RC_2015-09_all_.csv',\n",
      " 'all_RC_2015-10_all_.csv',\n",
      " 'all_RC_2015-11_all_.csv',\n",
      " 'all_RC_2015-12_all_.csv',\n",
      " 'all_RC_2016-01_all_.csv',\n",
      " 'all_RC_2016-02_all_.csv',\n",
      " 'all_RC_2016-03_all_.csv',\n",
      " 'all_RC_2016-04_all_.csv',\n",
      " 'all_RC_2016-05_all_.csv',\n",
      " 'all_RC_2016-06_all_.csv',\n",
      " 'all_RC_2016-07_all_.csv',\n",
      " 'all_RC_2016-08_all_.csv',\n",
      " 'all_RC_2016-09_all_.csv',\n",
      " 'all_RC_2016-10_all_.csv',\n",
      " 'all_RC_2016-11_all_.csv',\n",
      " 'all_RC_2016-12_ala_.csv',\n",
      " 'all_RC_2017-01_all_.csv',\n",
      " 'all_RC_2017-02_all_.csv',\n",
      " 'all_RC_2017-03_all_.csv',\n",
      " 'all_RC_2017-04_all_.csv',\n",
      " 'all_RC_2017-05_all_.csv',\n",
      " 'all_RC_2017-06_all_.csv',\n",
      " 'all_RC_2017-07_all_.csv',\n",
      " 'all_RC_2017-08_all_.csv',\n",
      " 'all_RC_2017-09_all_.csv',\n",
      " 'all_RC_2017-10_all_.csv',\n",
      " 'all_RC_2017-11_all_.csv',\n",
      " 'all_RC_2017-12_all_.csv',\n",
      " 'all_RC_2018-01_all_.csv',\n",
      " 'all_RC_2018-02_all_.csv',\n",
      " 'all_RC_2018-03_all_.csv',\n",
      " 'all_RC_2018-04_all_.csv',\n",
      " 'all_RC_2018-05_all_.csv',\n",
      " 'all_RC_2018-06_all_.csv',\n",
      " 'all_RC_2018-07_all_.csv',\n",
      " 'all_RC_2018-08_all_.csv',\n",
      " 'all_RC_2018-09_all_.csv',\n",
      " 'all_RC_2018-10_all_.csv',\n",
      " 'all_RC_2018-11_all_.csv',\n",
      " 'all_RC_2018-12_all_.csv']\n",
      "all_RC_2014-01_all_.csv file has {} corrupt rows!\n",
      "all_RC_2014-02_all_.csv file has {} corrupt rows!\n",
      "all_RC_2014-03_all_.csv file has {} corrupt rows!\n",
      "all_RC_2014-04_all_.csv file has {} corrupt rows!\n",
      "all_RC_2014-05_all_.csv file has {} corrupt rows!\n",
      "all_RC_2014-06_all_.csv file has {} corrupt rows!\n",
      "all_RC_2014-07_all_.csv file has {} corrupt rows!\n",
      "all_RC_2014-08_all_.csv file has {} corrupt rows!\n",
      "all_RC_2014-09_all_.csv file has {} corrupt rows!\n",
      "all_RC_2014-10_all_.csv file has {} corrupt rows!\n",
      "all_RC_2014-11_all_.csv file has {} corrupt rows!\n",
      "all_RC_2014-12_all_.csv file has {} corrupt rows!\n",
      "all_RC_2015-01_all_.csv file has {} corrupt rows!\n",
      "all_RC_2015-02_all_.csv file has {} corrupt rows!\n",
      "all_RC_2015-03_all_.csv file has {} corrupt rows!\n",
      "all_RC_2015-04_all_.csv file has {} corrupt rows!\n",
      "all_RC_2015-05_all_.csv file has {} corrupt rows!\n",
      "all_RC_2015-06_all_.csv file has {} corrupt rows!\n",
      "all_RC_2015-07_all_.csv file has {} corrupt rows!\n",
      "all_RC_2015-08_all_.csv file has {} corrupt rows!\n",
      "all_RC_2015-09_all_.csv file has {} corrupt rows!\n",
      "all_RC_2015-10_all_.csv file has {} corrupt rows!\n",
      "all_RC_2015-11_all_.csv file has {} corrupt rows!\n",
      "all_RC_2015-12_all_.csv file has {} corrupt rows!\n",
      "all_RC_2016-01_all_.csv file has {} corrupt rows!\n",
      "all_RC_2016-02_all_.csv file has {} corrupt rows!\n",
      "all_RC_2016-03_all_.csv file has {} corrupt rows!\n",
      "all_RC_2016-04_all_.csv file has {} corrupt rows!\n",
      "all_RC_2016-05_all_.csv file has {} corrupt rows!\n",
      "all_RC_2016-06_all_.csv file has {} corrupt rows!\n",
      "all_RC_2016-07_all_.csv file has {} corrupt rows!\n",
      "all_RC_2016-08_all_.csv file has {} corrupt rows!\n",
      "all_RC_2016-09_all_.csv file has {} corrupt rows!\n",
      "all_RC_2016-10_all_.csv file has {} corrupt rows!\n",
      "all_RC_2016-11_all_.csv file has {} corrupt rows!\n",
      "all_RC_2016-12_ala_.csv file has {} corrupt rows!\n",
      "all_RC_2017-01_all_.csv file has {} corrupt rows!\n",
      "all_RC_2017-02_all_.csv file has {} corrupt rows!\n",
      "all_RC_2017-03_all_.csv file has {} corrupt rows!\n",
      "all_RC_2017-04_all_.csv file has {} corrupt rows!\n",
      "all_RC_2017-05_all_.csv file has {} corrupt rows!\n",
      "all_RC_2017-06_all_.csv file has {} corrupt rows!\n",
      "all_RC_2017-07_all_.csv file has {} corrupt rows!\n",
      "all_RC_2017-08_all_.csv file has {} corrupt rows!\n",
      "all_RC_2017-09_all_.csv file has {} corrupt rows!\n",
      "all_RC_2017-10_all_.csv file has {} corrupt rows!\n",
      "all_RC_2017-11_all_.csv file has {} corrupt rows!\n",
      "all_RC_2017-12_all_.csv file has {} corrupt rows!\n",
      "all_RC_2018-01_all_.csv file has {} corrupt rows!\n",
      "all_RC_2018-02_all_.csv file has {} corrupt rows!\n",
      "all_RC_2018-03_all_.csv file has {} corrupt rows!\n",
      "all_RC_2018-04_all_.csv file has {} corrupt rows!\n",
      "all_RC_2018-05_all_.csv file has {} corrupt rows!\n",
      "all_RC_2018-06_all_.csv file has {} corrupt rows!\n",
      "all_RC_2018-07_all_.csv file has {} corrupt rows!\n",
      "all_RC_2018-08_all_.csv file has {} corrupt rows!\n",
      "all_RC_2018-09_all_.csv file has {} corrupt rows!\n",
      "all_RC_2018-10_all_.csv file has {} corrupt rows!\n",
      "all_RC_2018-11_all_.csv file has {} corrupt rows!\n",
      "all_RC_2018-12_all_.csv file has {} corrupt rows!\n",
      "'Number of rows for comments = 3677507'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>body</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>gilded</th>\n",
       "      <th>id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>permalink</th>\n",
       "      <th>removal_reason</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Uncentury</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As someone who lives and Memphis and sees FedE...</td>\n",
       "      <td>0</td>\n",
       "      <td>1389915441</td>\n",
       "      <td>0</td>\n",
       "      <td>cerg1q8</td>\n",
       "      <td>t3_1vck13</td>\n",
       "      <td>t3_1vck13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>dataisbeautiful</td>\n",
       "      <td>t5_2tk95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kerbuffel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There's music playing in the background</td>\n",
       "      <td>0</td>\n",
       "      <td>1389915509</td>\n",
       "      <td>0</td>\n",
       "      <td>cerg2tw</td>\n",
       "      <td>t3_1vck13</td>\n",
       "      <td>t1_cer6gho</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>dataisbeautiful</td>\n",
       "      <td>t5_2tk95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SlackingAlready</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Interesting, but keep in mind that this sample...</td>\n",
       "      <td>0</td>\n",
       "      <td>1389915629</td>\n",
       "      <td>0</td>\n",
       "      <td>cerg4rl</td>\n",
       "      <td>t3_1veeqv</td>\n",
       "      <td>t3_1veeqv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>dataisbeautiful</td>\n",
       "      <td>t5_2tk95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rand000m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"the owner of this video has not made it avail...</td>\n",
       "      <td>0</td>\n",
       "      <td>1389916005</td>\n",
       "      <td>0</td>\n",
       "      <td>cergasd</td>\n",
       "      <td>t3_1vck13</td>\n",
       "      <td>t3_1vck13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>dataisbeautiful</td>\n",
       "      <td>t5_2tk95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hockeyfan1133</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Correlation is not causation.</td>\n",
       "      <td>0</td>\n",
       "      <td>1389916117</td>\n",
       "      <td>0</td>\n",
       "      <td>cergcle</td>\n",
       "      <td>t3_1ve29c</td>\n",
       "      <td>t1_cerfej0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14</td>\n",
       "      <td>dataisbeautiful</td>\n",
       "      <td>t5_2tk95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### text files loading ###\n",
    "\n",
    "\n",
    "# function for dataframe checking ###\n",
    "\n",
    "def df_checking(df, file_dict, df_name):\n",
    "    count_dict = dict()\n",
    "    def check_timestamp(timestamp):\n",
    "        #     print(timestamp)\n",
    "        try:\n",
    "            datetime.fromtimestamp(int(timestamp))\n",
    "        except ValueError as e:\n",
    "            count_dict[str(timestamp)] = count_dict.get(str(timestamp), 0) + 1\n",
    "        except TypeError as e:\n",
    "            count_dict['None'] = count_dict.get('None', 0) + 1\n",
    "        \n",
    "    if file_dict['cols']['id'] not in df.columns:\n",
    "        raise ValueError('{} doesn\\'t have {} column!'.format(df_name, file_dict['cols']['id']))\n",
    "    if file_dict['cols']['date'] not in df.columns:\n",
    "        raise ValueError('{} doesn\\'t have date column!'.format(df_name, file_dict['cols']['date']))\n",
    "    if file_dict['cols']['category'] and file_dict['cols']['category'] not in df.columns:\n",
    "        raise ValueError('{} doesn\\'t have {} column!'.format(df_name, file_dict['cols']['category']))\n",
    "    for text_col in file_dict['cols']['text']:\n",
    "        if text_col not in df.columns:\n",
    "            raise ValueError('{} doesn\\'t have {} column!'.format(df_name, text_col))\n",
    "\n",
    "    if file_dict['formats']['date'] == 'timestamp':\n",
    "        count_dict = dict()\n",
    "        df[file_dict['cols']['date']].apply(check_timestamp)\n",
    "    else:\n",
    "        for date in df[file_dict['cols']['date']]:\n",
    "            try:\n",
    "                pd.to_datetime(date, format=file_dict['formats']['date'])\n",
    "            except ValueError as e:\n",
    "                count_dict[str(date)] = count_dict.get(str(date), 0) + 1\n",
    "\n",
    "    print('{} file has {} corrupt rows!'.format(df_name, count_dict))\n",
    "\n",
    "    \n",
    "dfs = dict()\n",
    "\n",
    "for file in c.template['files'].keys():\n",
    "\n",
    "    fname = list()\n",
    "\n",
    "    for f in os.listdir(c.directory['files'] + file):\n",
    "        if (f.find('.{}'.format(c.template['files'][file]['formats']['file'])) != -1 and f[0] != '.'):\n",
    "            fname.append(f)\n",
    "\n",
    "    fname=sorted(fname)\n",
    "    pprint(fname)\n",
    "\n",
    "### input file should be cleaned from \\r using the bash script\n",
    "\n",
    "    if len(fname)>0:\n",
    "        dfs[file] = pd.read_csv(c.directory['files'] + file + '/' + fname[0],\n",
    "                              sep=c.template['files'][file]['formats']['sep'])\n",
    "        df_checking(dfs[file], c.template['files'][file], fname[0])\n",
    "\n",
    "        for f in fname[1:]:\n",
    "            df_temp = pd.read_csv(c.directory['files'] + file + '/' + f,\n",
    "                                    sep=c.template['files'][file]['formats']['sep'],\n",
    "                                    skip_blank_lines=True\n",
    "                                    )\n",
    "            df_checking(df_temp, c.template['files'][file], f)\n",
    "            dfs[file] = pd.concat([dfs[file], df_temp], axis=0, sort=True)\n",
    "    else:\n",
    "        dfs[file]=pd.DataFrame() \n",
    "        \n",
    "\n",
    "    pprint('Number of rows for {} = {}'.format(file, len(dfs[file])))\n",
    "    util.display_df(dfs[file].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T20:35:40.621581Z",
     "start_time": "2019-05-14T20:35:40.538020Z"
    },
    "code_folding": [
     0
    ],
    "papermill": {
     "duration": 0.082526,
     "end_time": "2019-05-14T20:52:23.490751",
     "exception": false,
     "start_time": "2019-05-14T20:52:23.408225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'tags = 449'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>TagName</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>definitions</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>machine-learning</td>\n",
       "      <td>4766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>bigdata</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>data-mining</td>\n",
       "      <td>790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>databases</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'tags = 451'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>mini batch gradient descent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>microsoft bot framework</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>evolutionari algorithm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>social network analysi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>natur languag process</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### tags files loading, cleaning, stemming & saving ###\n",
    "\n",
    "if c.tags is not None:\n",
    "    ps = PorterStemmer()\n",
    "    tags = pd.read_csv(c.directory['tags'] + c.tags['name'])\n",
    "    pprint('tags = {}'.format(len(tags)))\n",
    "    util.display_df(tags.head())\n",
    "\n",
    "    tags_imported = list(tags[c.tags['col_name']])\n",
    "    tags_imported.extend(c.tags['extra'])\n",
    "\n",
    "    tags_list = [tag.replace('-', ' ') for tag in tags_imported]\n",
    "    \n",
    "    stemmed_tags_list=[]\n",
    "    for tags in tags_list:\n",
    "        res=[]\n",
    "        for tag_word in tags.split():\n",
    "            res.append(ps.stem(tag_word))\n",
    "        stemmed_tags_list.append(' '.join(res))\n",
    "    stemmed_tags_list=list(set(stemmed_tags_list))\n",
    "    stemmed_tags_list.sort(key=len, reverse=True)\n",
    "\n",
    "    tags_dict = {}\n",
    "    for tag_id, tag in enumerate(stemmed_tags_list):\n",
    "        tags_dict[tag_id] = tag\n",
    "    df_tags = pd.DataFrame(zip(tags_dict.keys(), tags_dict.values()), columns=['key', 'val'])\n",
    "    df_tags.to_csv('{}{}_0_tags.csv'.format(c.directory['save'], c.project_name), index=False)\n",
    "\n",
    "    pprint('tags = {}'.format(len(df_tags)))\n",
    "    util.display_df(df_tags.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T20:35:40.638012Z",
     "start_time": "2019-05-14T20:35:40.624142Z"
    },
    "code_folding": [
     0
    ],
    "papermill": {
     "duration": 0.043069,
     "end_time": "2019-05-14T20:52:23.567001",
     "exception": false,
     "start_time": "2019-05-14T20:52:23.523932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### functions for time correction ###\n",
    "\n",
    "def concat_cols(row):\n",
    "    return ' '.join(map(str, row))\n",
    "\n",
    "def concat_cols_df(df):\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        if len(df.columns) == 1:\n",
    "            return df\n",
    "        return df.apply(concat_cols, axis=1)\n",
    "    elif isinstance(df, pd.Series):\n",
    "        return df.apply(concat_cols)\n",
    "    raise Failed('Input was not either dataframe or series!')\n",
    "\n",
    "def str_timestamp_to_date(date):\n",
    "    try:\n",
    "        return datetime.fromtimestamp(int(date))\n",
    "    except ValueError as e:\n",
    "        util.count_sync(10)\n",
    "        return default_date\n",
    "\n",
    "def str_timestamp_to_date_df(df):\n",
    "    return df.apply(str_timestamp_to_date)\n",
    "\n",
    "def str_timestamp_to_date_df_dask(df):\n",
    "    return df['date'].apply(str_timestamp_to_date)\n",
    "\n",
    "def str_date_to_date(date):\n",
    "    try:\n",
    "        return pd.to_datetime(df_text['date'], format=c.template['files'][df_name]['formats']['date'])\n",
    "    except ValueError as e:\n",
    "        util.print_asynch(date)\n",
    "        return default_date\n",
    "\n",
    "\n",
    "def date_to_timestamp(date):\n",
    "    try:\n",
    "        return int(date.replace(tzinfo=timezone.utc).timestamp())\n",
    "    except Exception as e:\n",
    "        raise util.Failed('{}\\n{} has type with {} value'.format(e, type(date), date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T20:35:41.202505Z",
     "start_time": "2019-05-14T20:35:40.641504Z"
    },
    "code_folding": [
     0
    ],
    "papermill": {
     "duration": 47.437808,
     "end_time": "2019-05-14T20:53:11.037828",
     "exception": false,
     "start_time": "2019-05-14T20:52:23.600020",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'files name = submissions'\n",
      "parallel: 27 partitions with 27 cores for concat_cols_df\n",
      "parallel: 27 partitions with 27 cores for str_timestamp_to_date_df\n",
      "str_timestamp_to_date step is done!\n",
      "'New length for submissions = 226134'\n",
      "'files name = comments'\n",
      "parallel: 27 partitions with 27 cores for concat_cols_df\n",
      "parallel: 27 partitions with 27 cores for str_timestamp_to_date_df\n",
      "str_timestamp_to_date step is done!\n",
      "'New length for comments = 3333568'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>category</th>\n",
       "      <th>all_text</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2qzibe</td>\n",
       "      <td>1420090180</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>Measuring human performance on standard image ...</td>\n",
       "      <td>submissions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2qzppu</td>\n",
       "      <td>1420096683</td>\n",
       "      <td>dataisbeautiful</td>\n",
       "      <td>Bowls of Cereal I Ate in 2014 [OC]</td>\n",
       "      <td>submissions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2qzslb</td>\n",
       "      <td>1420099668</td>\n",
       "      <td>dataisbeautiful</td>\n",
       "      <td>Happy New Year Message</td>\n",
       "      <td>submissions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2qzv35</td>\n",
       "      <td>1420102342</td>\n",
       "      <td>dataisbeautiful</td>\n",
       "      <td>European economy guide: Taking Europe’s pulse</td>\n",
       "      <td>submissions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2qzxp9</td>\n",
       "      <td>1420105645</td>\n",
       "      <td>dataisbeautiful</td>\n",
       "      <td>Anchorage, Alaska never saw a day below zero i...</td>\n",
       "      <td>submissions</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>category</th>\n",
       "      <th>all_text</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3559697</th>\n",
       "      <td>ecxh5wv</td>\n",
       "      <td>1546237750</td>\n",
       "      <td>dataisbeautiful</td>\n",
       "      <td>Ok, but the very rare exceptions don't really ...</td>\n",
       "      <td>comments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3559698</th>\n",
       "      <td>ecxhdd9</td>\n",
       "      <td>1546237967</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>Thanks! I have a copy of that book - maybe I s...</td>\n",
       "      <td>comments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3559699</th>\n",
       "      <td>ecxhihv</td>\n",
       "      <td>1546238119</td>\n",
       "      <td>dataisbeautiful</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>comments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3559700</th>\n",
       "      <td>ecxhteb</td>\n",
       "      <td>1546238455</td>\n",
       "      <td>dataisbeautiful</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>comments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3559701</th>\n",
       "      <td>ecxi0nd</td>\n",
       "      <td>1546238689</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>Hi ranihorev,\\n\\n\\nUsing the same values for n...</td>\n",
       "      <td>comments</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### text concatenation + date correction + date filtering (min & max) & saving ###\n",
    "\n",
    "dfs_text = pd.DataFrame()\n",
    "\n",
    "for df_name in c.template['files']:\n",
    "    # df_name='submissions'\n",
    "    if(len(dfs[df_name])==0):\n",
    "        continue\n",
    "    pprint('files name = {}'.format(df_name))\n",
    "    df = c.template['files'][df_name]['cols']\n",
    "    cols = [df['id']]\n",
    "    cols.append(df['date'])\n",
    "    if df['category']:\n",
    "        cols.append(df['category'])\n",
    "    cols.extend(df['text'])\n",
    "    df_text = dfs[df_name][cols]\n",
    "    df_text = df_text.fillna(value='')\n",
    "    df_text.reset_index(inplace=True)\n",
    "    \n",
    "#     util.init_parallel(False, c)\n",
    "#     df_text['all_text'] = df_text[df['text']].parallel_apply(concat_cols, axis=1)\n",
    "#     util.display_df(df_text[df['text']].head(30))\n",
    "#     util.display_df(df_text[df['text']].tail())\n",
    "    df_text['all_text'] = util.parallelize_df(df_text[df['text']], concat_cols_df, c)\n",
    "#     print(df_text['all_text'])\n",
    "    \n",
    "    \n",
    "    df_text.drop(df['text'], axis=1, inplace=True)\n",
    "    df_text.rename(columns={\n",
    "        df['id']: 'id',\n",
    "        df['date']: 'date',\n",
    "    }, inplace=True)\n",
    "    if df['category']:\n",
    "        df_text.rename(columns={df['category']: 'category'}, inplace=True)\n",
    "    min_date = pd.to_datetime(c.min_date_val, format=c.date_format)\n",
    "    max_date = pd.to_datetime(c.max_date_val, format=c.date_format)\n",
    "    default_date = min_date - timedelta(days=10)\n",
    "\n",
    "    \n",
    "    if c.template['files'][df_name]['formats']['date'] == 'timestamp':\n",
    "#         util.init_parallel(False, c)\n",
    "#         df_text['date_converted'] = df_text['date'].apply(str_timestamp_to_date)\n",
    "#         df_text['date_converted'] = util.parallelize_df(df_text['date'], str_timestamp_to_date_df, c)\n",
    "        df_text['date_converted'] = util.parallelize_df_dask(df_text['date'], str_timestamp_to_date_df, c)    \n",
    "#         df_text['date_converted'] = df_text['date'].apply(str_timestamp_to_date)    \n",
    "        print('str_timestamp_to_date step is done!')\n",
    "    else:\n",
    "        df_text['date_converted'] = util.parallelize_df(df_text['date'], str_date_to_date, c)\n",
    "#         df_text['date_converted'] = util.parallelize_df_dask(df_text['date'], str_date_to_date, c)        \n",
    "        print('str_date_to_date step is done!')\n",
    "\n",
    "        # df_text['date']=c.parallelize_df(c.copy.deepcopy(df_text['date_converted']), date_to_timestamp)\n",
    "        # print(type(df_text['date_converted'][0]))\n",
    "        df_text['date'] = df_text['date_converted'].apply(date_to_timestamp)\n",
    "        print('int_date_to_timestamp step is done!')\n",
    "\n",
    "    df_text = df_text[(df_text['date_converted'] <= max_date) & (df_text['date_converted'] >= min_date)]\n",
    "\n",
    "    df_text.drop(['date_converted'], axis=1, inplace=True)\n",
    "    df_text.reset_index(drop=True, inplace=True)\n",
    "    df_text['group'] = df_name\n",
    "    dfs_text = pd.concat([dfs_text, df_text], ignore_index=True, axis=0)\n",
    "    pprint('New length for {} = {}'.format(df_name, len(df_text)))\n",
    "\n",
    "dfs_text.drop(['index'], axis=1, inplace=True)\n",
    "dfs_text.to_csv('{}{}_1_text_original.csv'.format(c.directory['save'], c.project_name), index=False)\n",
    "\n",
    "util.display_df(dfs_text.head())\n",
    "util.display_df(dfs_text.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T20:35:41.216797Z",
     "start_time": "2019-05-14T20:35:41.207019Z"
    },
    "code_folding": [
     0
    ],
    "papermill": {
     "duration": 0.045319,
     "end_time": "2019-05-14T20:53:11.119927",
     "exception": false,
     "start_time": "2019-05-14T20:53:11.074608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### functions for cleaning ###\n",
    "\n",
    "def preprocess_text(text):\n",
    "    for replacement in c.replacements:\n",
    "        try:\n",
    "            text = re.sub(*replacement, text)\n",
    "        except TypeError as e:\n",
    "            print('{} + text {}'.format(replacement, text))\n",
    "            raise e\n",
    "#     return tokenizer_tag(text)\n",
    "    return text\n",
    "\n",
    "def preprocess_text_wo_tokenization_df(df):\n",
    "    return df.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T20:35:41.480436Z",
     "start_time": "2019-05-14T20:35:41.220730Z"
    },
    "code_folding": [
     0
    ],
    "papermill": {
     "duration": 58.353357,
     "end_time": "2019-05-14T20:54:09.507504",
     "exception": false,
     "start_time": "2019-05-14T20:53:11.154147",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parallel: 27 partitions with 27 cores for preprocess_text_wo_tokenization_df\n",
      "'size of all texts = (3559702, 5)'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>category</th>\n",
       "      <th>all_text</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2qzibe</td>\n",
       "      <td>1420090180</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>Measuring human performance on standard image ...</td>\n",
       "      <td>submissions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2qzppu</td>\n",
       "      <td>1420096683</td>\n",
       "      <td>dataisbeautiful</td>\n",
       "      <td>Bowls of Cereal I Ate in 2014</td>\n",
       "      <td>submissions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2qzslb</td>\n",
       "      <td>1420099668</td>\n",
       "      <td>dataisbeautiful</td>\n",
       "      <td>Happy New Year Message</td>\n",
       "      <td>submissions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2qzv35</td>\n",
       "      <td>1420102342</td>\n",
       "      <td>dataisbeautiful</td>\n",
       "      <td>European economy guide: Taking Europe’s pulse</td>\n",
       "      <td>submissions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2qzxp9</td>\n",
       "      <td>1420105645</td>\n",
       "      <td>dataisbeautiful</td>\n",
       "      <td>Anchorage, Alaska never saw a day below zero i...</td>\n",
       "      <td>submissions</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### cleaning using rules in the config file + saving ###\n",
    "\n",
    "# util.init_parallel(True, c)\n",
    "# dfs_text['all_text'] = dfs_text['all_text'].parallel_apply(preprocess_text)\n",
    "# dfs_text['all_text'] = util.parallelize_df(dfs_text['all_text'], preprocess_text_df, c)\n",
    "# dfs_text['all_text'] = util.parallelize_df_dask(dfs_text['all_text'], preprocess_text_df, c)\n",
    "dfs_text['all_text'] = util.parallelize_df_dask(dfs_text['all_text'], preprocess_text_wo_tokenization_df, c)\n",
    "\n",
    "\n",
    "# saving the result to file\n",
    "\n",
    "dfs_text.to_csv('{}{}_2_text_cleaned.csv'.format(c.directory['save'], c.project_name), index=False)\n",
    "\n",
    "pprint('size of all texts = {}'.format(dfs_text.shape))\n",
    "util.display_df(dfs_text.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T20:35:41.532215Z",
     "start_time": "2019-05-14T20:35:41.483490Z"
    },
    "code_folding": [
     0,
     2
    ],
    "papermill": {
     "duration": 22.611259,
     "end_time": "2019-05-14T20:54:32.186370",
     "exception": false,
     "start_time": "2019-05-14T20:54:09.575111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3559702, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>category</th>\n",
       "      <th>all_text</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2qzibe</td>\n",
       "      <td>1420090180</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>Measuring human performance on standard image ...</td>\n",
       "      <td>submissions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2qzppu</td>\n",
       "      <td>1420096683</td>\n",
       "      <td>dataisbeautiful</td>\n",
       "      <td>Bowls of Cereal I Ate in 2014</td>\n",
       "      <td>submissions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2qzslb</td>\n",
       "      <td>1420099668</td>\n",
       "      <td>dataisbeautiful</td>\n",
       "      <td>Happy New Year Message</td>\n",
       "      <td>submissions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2qzv35</td>\n",
       "      <td>1420102342</td>\n",
       "      <td>dataisbeautiful</td>\n",
       "      <td>European economy guide: Taking Europe’s pulse</td>\n",
       "      <td>submissions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2qzxp9</td>\n",
       "      <td>1420105645</td>\n",
       "      <td>dataisbeautiful</td>\n",
       "      <td>Anchorage, Alaska never saw a day below zero i...</td>\n",
       "      <td>submissions</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### cleaned & tagged text loading + tag file loading ###\n",
    "\n",
    "if c.tags is not None:\n",
    "    df_tags = pd.read_csv('{}{}_0_tags.csv'.format(c.directory['save'], c.project_name))\n",
    "    c.tags_dict = dict(zip(df_tags['key'], df_tags['val']))\n",
    "\n",
    "df_text_orig = pd.read_csv('{}{}_1_text_original.csv'.format(c.directory['save'], c.project_name))\n",
    "df_text = pd.read_csv('{}{}_2_text_cleaned.csv'.format(c.directory['save'], c.project_name))\n",
    "\n",
    "pprint(df_text.shape)\n",
    "util.display_df(df_text.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T20:35:42.515634Z",
     "start_time": "2019-05-14T20:35:41.537542Z"
    },
    "code_folding": [
     0,
     8,
     18,
     33,
     62,
     67,
     72,
     75,
     78,
     81,
     86
    ],
    "papermill": {
     "duration": 0.592992,
     "end_time": "2019-05-14T20:54:32.879606",
     "exception": false,
     "start_time": "2019-05-14T20:54:32.286614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### functions for unigram, bigram and tokenization ###\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(c.stop_words_extensions)\n",
    "nlp = spacy.load(\"en\")\n",
    "nlp.add_pipe(LanguageDetector(), name=\"language_detector\", last=True)\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def num_convertor(num):\n",
    "    if num == 0:\n",
    "        return 'lzerol'\n",
    "    num_string = 'l'\n",
    "\n",
    "    while num != 0:\n",
    "        num_string = 'l{}{}'.format(c.num_dict[num % 10], num_string)\n",
    "        num //= 10\n",
    "    return num_string\n",
    "\n",
    "def tokenizer_tag(text):\n",
    "    #     print(text)\n",
    "    if c.tags is None:\n",
    "        return text\n",
    "    txt = ' ' + text.lower() + ' '\n",
    "\n",
    "    to_remove = string.punctuation.replace('\\'', '').replace('_', '')\n",
    "    table = {ord(char): ord(' ') for char in to_remove}\n",
    "    txt = txt.translate(table)\n",
    "    if txt.find('zzz') > -1:\n",
    "        return ' '\n",
    "    for ind in range(len(c.tags_dict)):\n",
    "        txt = txt.replace(' ' + c.tags_dict[ind] + ' ', ' zzz' + num_convertor(ind) + 'zzz ')\n",
    "    return txt\n",
    "\n",
    "def tokenizer_tag_new(text):\n",
    "    #     print(text)\n",
    "    if c.tags is None:\n",
    "        return text\n",
    "    txt = ' ' + text.lower() + ' '\n",
    "\n",
    "    to_remove = string.punctuation.replace('\\'', '').replace('_', '')\n",
    "    table = {ord(char): ord(' ') for char in to_remove}\n",
    "    txt = txt.translate(table)\n",
    "    \n",
    "#     print(txt)\n",
    "    bigrams = re.findall('\\w+_\\w+', txt)    \n",
    "    bigrams_dict={}\n",
    "    for bigram in bigrams:\n",
    "        bigrams_dict[bigram.replace('_',' ')]=bigram\n",
    "#     print(bigrams_dict)\n",
    "    txt=txt.replace('_', ' ')\n",
    "#     print(txt)\n",
    "\n",
    "    if txt.find('zzz') > -1:\n",
    "        return ' '\n",
    "    for ind in range(len(c.tags_dict)):\n",
    "        txt = txt.replace(' ' + c.tags_dict[ind] + ' ', ' zzz' + num_convertor(ind) + 'zzz ')\n",
    "\n",
    "    for bigram in bigrams_dict:\n",
    "        txt=txt.replace(bigram, bigrams_dict[bigram])\n",
    "    \n",
    "    return txt\n",
    "\n",
    "def english_detection_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    # document level language detection. Think of it like average language of document!\n",
    "    return doc._.language['language']=='en'\n",
    "\n",
    "def para_words_english_spacy(text):\n",
    "    if c.lang_detect and not english_detection_spacy(text):\n",
    "        return []\n",
    "    return [ps.stem(word) for word in simple_preprocess(text, min_len=1, max_len=60, deacc=True) if word not in stop_words]\n",
    "\n",
    "def para_words_english_spacy_df(df):\n",
    "    return df.apply(para_words_english_spacy)   \n",
    "\n",
    "def para_bigram(text):\n",
    "    return bigram_mod[text]\n",
    "\n",
    "def para_bigram_df(df):\n",
    "    return df.apply(para_bigram)\n",
    "\n",
    "def para_bigram_tokenization(text):\n",
    "#     text = tokenizer_tag(' '.join(bigram_mod[text]))\n",
    "    text = tokenizer_tag_new(' '.join(bigram_mod[text]))    \n",
    "    return text.split()\n",
    "\n",
    "def para_bigram_tokenization_df(df):\n",
    "    return df.apply(para_bigram_tokenization)\n",
    "\n",
    "# def make_trigrams(texts):\n",
    "#     return [trigram_mod[bigram_mod[doc]] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T20:36:21.622093Z",
     "start_time": "2019-05-14T20:35:42.517757Z"
    },
    "code_folding": [
     0
    ],
    "papermill": {
     "duration": 5385.89011,
     "end_time": "2019-05-14T22:24:18.808007",
     "exception": false,
     "start_time": "2019-05-14T20:54:32.917897",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parallel: 27 partitions with 27 cores for para_words_english_spacy_df\n",
      "parallel: 27 partitions with 27 cores for para_bigram_tokenization_df\n"
     ]
    }
   ],
   "source": [
    "### Build the bigram model ###\n",
    "\n",
    "# util.init_parallel(True)\n",
    "# words = df_text['all_text'].parallel_apply(para_words)\n",
    "# words = util.parallelize_df(df_text['all_text'], para_words_df, c)\n",
    "# words = util.parallelize_df_dask(df_text['all_text'], para_words_df, c)\n",
    "# en_lang=english_lang(stop_words)\n",
    "# words = en_lang.parallelize_english_detection_df(df_text['all_text'], c)\n",
    "words = util.parallelize_df_dask(df_text['all_text'], para_words_english_spacy_df, c)\n",
    "\n",
    "bigram = gensim.models.Phrases(words, min_count=10, threshold=10)  # higher threshold fewer phrases.\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "# c.init_parallel(True)\n",
    "# data_words_bigrams = words.parallel_apply(para_bigram)\n",
    "# data_words_bigrams = util.parallelize_df(words, para_bigram_df, c)\n",
    "# data_words_bigrams = util.parallelize_df_dask(words, para_bigram_df, c)\n",
    "data_words_bigrams = util.parallelize_df_dask(words, para_bigram_tokenization_df, c)\n",
    "\n",
    "# trigram = gensim.models.Phrases(bigram[words], threshold=1)\n",
    "# trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T20:36:42.088122Z",
     "start_time": "2019-05-14T20:36:21.625915Z"
    },
    "code_folding": [
     0,
     5
    ],
    "papermill": {
     "duration": 678.459526,
     "end_time": "2019-05-14T22:35:37.312376",
     "exception": false,
     "start_time": "2019-05-14T22:24:18.852850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parallel: 27 partitions with 27 cores for para_lemmatization_part\n"
     ]
    }
   ],
   "source": [
    "### lemmatization ###\n",
    "\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "# ps = PorterStemmer()\n",
    "\n",
    "def para_lemmatization_part(df, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    result = []\n",
    "\n",
    "    for text in list(df['words']):\n",
    "        if len(' '.join(text)) != 0:\n",
    "            doc = nlp(' '.join(text))\n",
    "#             result.append([ps.stem(token.lemma_.replace('_', '')) for token in doc if token.pos_ in allowed_postags])\n",
    "            result.append([token.lemma_.replace('_', '') for token in doc if token.pos_ in allowed_postags])        \n",
    "        else:\n",
    "            result.append(list())\n",
    "    df['words'] = result\n",
    "    return df\n",
    "\n",
    "\n",
    "df_words = pd.DataFrame(dict(words=data_words_bigrams))\n",
    "# data_lemmatized = util.parallelize_df(df_words, para_lemmatization_part, c)\n",
    "data_lemmatized = util.parallelize_df_dask(df_words, para_lemmatization_part, c)\n",
    "data_lemmatized = data_lemmatized['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T20:36:42.350354Z",
     "start_time": "2019-05-14T20:36:42.090689Z"
    },
    "code_folding": [
     0
    ],
    "papermill": {
     "duration": 101.726262,
     "end_time": "2019-05-14T22:37:19.119376",
     "exception": false,
     "start_time": "2019-05-14T22:35:37.393114",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parallel: 27 partitions with 27 cores for para_corpus_df\n",
      "'After thresholding = 2225657'\n"
     ]
    }
   ],
   "source": [
    "### Thresholding: delete text less than the minimum threshold ###\n",
    "\n",
    "###   7-1 min < word < % of corupus\n",
    "\n",
    "dct = corpora.Dictionary(data_lemmatized)\n",
    "dct.filter_extremes(no_below=c.no_below, no_above=c.no_above)\n",
    "# corpus = [ for text in data_lemmatized]\n",
    "def para_corpus(text):\n",
    "    return len(dct.doc2bow(text))\n",
    "\n",
    "def para_corpus_df(df):\n",
    "    return df.apply(para_corpus)\n",
    "\n",
    "corpus=util.parallelize_df(data_lemmatized, para_corpus_df, c)\n",
    "\n",
    "###     7-2 min_words_per_docs\n",
    "\n",
    "data_untagged = df_text_orig['all_text'].to_list()\n",
    "count = 0\n",
    "ids = []\n",
    "df_temp = []\n",
    "data_temp = []\n",
    "\n",
    "\n",
    "count = 0\n",
    "ids = []\n",
    "df_temp = []\n",
    "data_temp = []\n",
    "\n",
    "for i, x in enumerate(corpus):\n",
    "    if x >= c.min_keyword_threshold:\n",
    "        df_temp.append(x)\n",
    "        data_temp.append(data_untagged[i])\n",
    "        ids.append(i)\n",
    "        count += 1\n",
    "\n",
    "data_lemmatized = data_lemmatized[ids]\n",
    "data_lemmatized.reset_index(drop=True, inplace=True)\n",
    "df_text = df_text.iloc[ids]\n",
    "df_text.reset_index(drop=True, inplace=True)\n",
    "data_untagged = data_temp\n",
    "pprint('After thresholding = {}'.format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T20:36:42.391974Z",
     "start_time": "2019-05-14T20:36:42.356442Z"
    },
    "code_folding": [
     0
    ],
    "papermill": {
     "duration": 48.13868,
     "end_time": "2019-05-14T22:38:07.298641",
     "exception": false,
     "start_time": "2019-05-14T22:37:19.159961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2225657, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>category</th>\n",
       "      <th>all_text</th>\n",
       "      <th>group</th>\n",
       "      <th>txt_orig</th>\n",
       "      <th>txt_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2qzibe</td>\n",
       "      <td>1420090180</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>Measuring human performance on standard image ...</td>\n",
       "      <td>submissions</td>\n",
       "      <td>Measuring human performance on standard image ...</td>\n",
       "      <td>measur, human, zzzltwolfivelonelzzz, standard,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2qzv35</td>\n",
       "      <td>1420102342</td>\n",
       "      <td>dataisbeautiful</td>\n",
       "      <td>European economy guide: Taking Europe’s pulse</td>\n",
       "      <td>submissions</td>\n",
       "      <td>European economy guide: Taking Europe’s pulse</td>\n",
       "      <td>european, economi, guid, take, europ, pul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2qzzaa</td>\n",
       "      <td>1420107805</td>\n",
       "      <td>dataisbeautiful</td>\n",
       "      <td>Popularity of visual forms in DataIsBeautiful ...</td>\n",
       "      <td>submissions</td>\n",
       "      <td>Popularity of visual forms in DataIsBeautiful ...</td>\n",
       "      <td>popular, zzzlthreelzerolfourlzzz, form, datais...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2r048c</td>\n",
       "      <td>1420114575</td>\n",
       "      <td>dataisbeautiful</td>\n",
       "      <td>JOE GROOMING - DAILY SHAMPOO ~ best shampoo fo...</td>\n",
       "      <td>submissions</td>\n",
       "      <td>JOE GROOMING - DAILY SHAMPOO ~ best shampoo fo...</td>\n",
       "      <td>joe, groom, daili, shampoo, good, shampoo, oil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2r0e7b</td>\n",
       "      <td>1420125946</td>\n",
       "      <td>dataisbeautiful</td>\n",
       "      <td>Los Angeles Traffic Accident Rate in Rainy vs ...</td>\n",
       "      <td>submissions</td>\n",
       "      <td>Los Angeles Traffic Accident Rate in Rainy vs ...</td>\n",
       "      <td>trafficaccid, rate, raini, dri, weather</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### concatanating + saving ###\n",
    "\n",
    "\n",
    "df_text_concat = pd.concat([\n",
    "    df_text,\n",
    "    pd.DataFrame(data_untagged),\n",
    "    pd.DataFrame([', '.join(x) for x in data_lemmatized]),\n",
    "],\n",
    "    ignore_index=True,\n",
    "    axis=1\n",
    ")\n",
    "df_cols = list(df_text_orig.columns)\n",
    "df_cols.extend(['txt_orig', 'txt_lemmatized', ])\n",
    "df_text_concat.columns = df_cols\n",
    "df_text_concat.to_csv('{}{}_3_text_lemmatized_tagged.csv'.format(c.directory['save'], c.project_name),\n",
    "                      encoding='utf-8',\n",
    "                      index=False\n",
    "                      )\n",
    "\n",
    "pprint(df_text_concat.shape)\n",
    "util.display_df(df_text_concat.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T20:36:42.399905Z",
     "start_time": "2019-05-14T20:36:42.394757Z"
    },
    "code_folding": [
     0,
     11,
     28
    ],
    "papermill": {
     "duration": 0.044446,
     "end_time": "2019-05-14T22:38:07.404410",
     "exception": false,
     "start_time": "2019-05-14T22:38:07.359964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ### removing the text under threshold + before & after lemmatization ###\n",
    "\n",
    "\n",
    "# # delete texts less than the minimum threshold\n",
    "\n",
    "# data_untagged = df_text_orig['all_text'].to_list()\n",
    "# count = 0\n",
    "# ids = []\n",
    "# df_temp = []\n",
    "# data_temp = []\n",
    "\n",
    "# for i, x in enumerate(data_words_bigrams):\n",
    "#     if len(x) >= c.min_keyword_threshold:\n",
    "#         df_temp.append(x)\n",
    "#         data_temp.append(data_untagged[i])\n",
    "#         ids.append(i)\n",
    "#         count += 1\n",
    "\n",
    "# data_words_bigrams = df_temp\n",
    "# # df_text = df_text.loc[ids, :]\n",
    "# df_text = df_text.iloc[ids]\n",
    "# df_text.reset_index(drop=True, inplace=True)\n",
    "# data_untagged = data_temp\n",
    "# pprint('After data cleaning = {}'.format(count))\n",
    "\n",
    "# nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "# # ps = PorterStemmer()\n",
    "\n",
    "# def para_lemmatization_part(df, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "#     result = []\n",
    "\n",
    "#     for text in list(df['words']):\n",
    "#         if len(' '.join(text)) != 0:\n",
    "#             doc = nlp(' '.join(text))\n",
    "# #             result.append([ps.stem(token.lemma_.replace('_', '')) for token in doc if token.pos_ in allowed_postags])\n",
    "#             result.append([token.lemma_.replace('_', '') for token in doc if token.pos_ in allowed_postags])        \n",
    "#         else:\n",
    "#             result.append(list())\n",
    "#     df['words'] = result\n",
    "#     return df\n",
    "\n",
    "\n",
    "# df_words = pd.DataFrame(dict(words=data_words_bigrams))\n",
    "# # data_lemmatized = util.parallelize_df(df_words, para_lemmatization_part, c)\n",
    "# data_lemmatized = util.parallelize_df_dask(df_words, para_lemmatization_part, c)\n",
    "# data_lemmatized = data_lemmatized['words']\n",
    "\n",
    "# ### delete lemmatized text less than the minimum threshold ###\n",
    "\n",
    "# count = 0\n",
    "# ids = []\n",
    "# df_temp = []\n",
    "# data_temp = []\n",
    "\n",
    "# for i, x in enumerate(data_lemmatized):\n",
    "#     if len(x) >= c.min_keyword_threshold:\n",
    "#         df_temp.append(x)\n",
    "#         data_temp.append(data_untagged[i])\n",
    "#         ids.append(i)\n",
    "#         count += 1\n",
    "\n",
    "# data_lemmatized = df_temp\n",
    "# # df_text = df_text.loc[ids, :]\n",
    "# df_text = df_text.iloc[ids]\n",
    "# df_text.reset_index(drop=True, inplace=True)\n",
    "# data_untagged = data_temp\n",
    "# pprint('After lemmatization = {}'.format(count))\n",
    "\n",
    "# df_text_concat = pd.concat([\n",
    "#     df_text,\n",
    "#     pd.DataFrame(data_untagged),\n",
    "#     pd.DataFrame([', '.join(x) for x in data_lemmatized]),\n",
    "# ],\n",
    "#     ignore_index=True,\n",
    "#     axis=1\n",
    "# )\n",
    "# df_cols = list(df_text_orig.columns)\n",
    "# df_cols.extend(['txt_orig', 'txt_lemmatized', ])\n",
    "# df_text_concat.columns = df_cols\n",
    "# df_text_concat.to_csv('{}{}_3_text_lemmatized_tagged.csv'.format(c.directory['save'], c.project_name),\n",
    "#                       encoding='utf-8',\n",
    "#                       index=False\n",
    "#                       )\n",
    "\n",
    "# pprint(df_text_concat.shape)\n",
    "# util.display_df(df_text_concat.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "papermill": {
   "duration": 6534.66024,
   "end_time": "2019-05-14T22:39:16.426005",
   "environment_variables": {},
   "exception": true,
   "input_path": "LDA_1_Text_Prep.ipynb",
   "output_path": "/scratch/hkarbasi/LDA/Reddit/LDA_1_Text_Prep_0_Reddit_NODE077.ipynb",
   "parameters": {
    "config": "configs.config_Reddit",
    "mem": 650000,
    "num_cores": 27
   },
   "start_time": "2019-05-14T20:50:21.765765",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
