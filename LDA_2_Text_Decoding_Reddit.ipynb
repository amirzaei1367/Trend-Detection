{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ],
    "papermill": {
     "duration": 0.012878,
     "end_time": "2019-05-17T16:09:51.859353",
     "exception": false,
     "start_time": "2019-05-17T16:09:51.846475",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_cores = 27\n",
    "mem = 650000\n",
    "config = \"configs.config_Reddit\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T15:01:02.206062Z",
     "start_time": "2019-05-17T15:01:02.201706Z"
    },
    "code_folding": [
     0
    ],
    "papermill": {
     "duration": 0.01057,
     "end_time": "2019-05-17T16:09:51.875616",
     "exception": false,
     "start_time": "2019-05-17T16:09:51.865046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### The steps of preprocessing:\n",
    "\n",
    "# 1- cleaning\n",
    "# 2- Unigram\n",
    "#     2-1 - English\n",
    "#     2-2 - Stop Words\n",
    "#     2-3 - Stemming\n",
    "# 3- Bigram\n",
    "# 4- Tokenization\n",
    "# 6- Lemmatization\n",
    "# 7- Thresholding: \n",
    "#     7-1 min < word < % of corupus\n",
    "#     7-2 min_words_per_docs\n",
    "# 8- LDA\n",
    "\n",
    "### The steps of preprocessing for tokens:\n",
    "\n",
    "# 1- cleaning\n",
    "# 2- Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T15:01:02.213055Z",
     "start_time": "2019-05-17T15:01:02.209781Z"
    },
    "code_folding": [
     0
    ],
    "papermill": {
     "duration": 0.010265,
     "end_time": "2019-05-17T16:09:51.891161",
     "exception": false,
     "start_time": "2019-05-17T16:09:51.880896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "if 'config' not in locals():\n",
    "    config = 'configs.config_SE_test_local'\n",
    "#     config = 'configs.config_Reddit_test_local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T15:01:08.232820Z",
     "start_time": "2019-05-17T15:01:02.220734Z"
    },
    "code_folding": [
     0
    ],
    "papermill": {
     "duration": 7.343704,
     "end_time": "2019-05-17T16:09:59.240199",
     "exception": false,
     "start_time": "2019-05-17T16:09:51.896495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config file is set to configs.config_Reddit\n",
      "num_cores is set to 27\n",
      "mem is set to 650000\n"
     ]
    }
   ],
   "source": [
    "### initialization ###\n",
    "\n",
    "from _imports import *\n",
    "# from _utils import *\n",
    "\n",
    "print('config file is set to {}'.format(config))\n",
    "\n",
    "util=importlib.import_module('_utils')\n",
    "importlib.reload(util)\n",
    "\n",
    "c=importlib.import_module(config)\n",
    "importlib.reload(c)\n",
    "\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "# param_setup(sys.argv[1:], c)\n",
    "util.param_setup_ipython(globals(), c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T15:01:08.252667Z",
     "start_time": "2019-05-17T15:01:08.235457Z"
    },
    "code_folding": [
     0
    ],
    "papermill": {
     "duration": 0.017579,
     "end_time": "2019-05-17T16:09:59.264126",
     "exception": false,
     "start_time": "2019-05-17T16:09:59.246547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Functions for decoding and date conversion ###\n",
    "\n",
    "\n",
    "def parallel_decode(list_tags):\n",
    "    try:\n",
    "#         return ', '.join(list(map(util.decode_tag, df)))\n",
    "        return ', '.join([util.decode_tag(tag.strip(), c) for tag in list_tags.split(',')])    \n",
    "    except Exception as e:\n",
    "#         text = ', '.join(list(list_tags))\n",
    "        util.print_asynch(list_tags)\n",
    "        raise util.Failed('couldn\\'t find {}\\n{}!'.format(list_tags), e)\n",
    "\n",
    "def parallel_decode_df(df):\n",
    "    return df.apply(parallel_decode)\n",
    "\n",
    "\n",
    "def parallel_decode_full(df):\n",
    "    try:\n",
    "#         return ', '.join(list(map(util.decode_tag, df)))\n",
    "        return ', '.join([util.decode_tag(tag.strip(), c) for tag in df['txt_lemmatized'].split(',')])    \n",
    "    except Exception as e:\n",
    "        util.print_asynch(df)\n",
    "        raise util.Failed('couldn\\'t find {}\\n{}!'.format(df), e)\n",
    "\n",
    "\n",
    "def parallel_decode_df_full(df):\n",
    "    return df.apply(parallel_decode_full, axis=1)\n",
    "\n",
    "\n",
    "def str_timestamp_to_date(date):\n",
    "    try:\n",
    "        return datetime.fromtimestamp(int(date))\n",
    "    except ValueError as e:\n",
    "#         util.count_sync(10)\n",
    "        raise Failed('{} couldn\\'t be converted. \\n {}'.format(date, e))\n",
    "\n",
    "def str_timestamp_to_date_df(df):\n",
    "    return df.apply(str_timestamp_to_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T15:01:23.047188Z",
     "start_time": "2019-05-17T15:01:08.254779Z"
    },
    "code_folding": [
     0,
     23,
     34,
     36
    ],
    "papermill": {
     "duration": 144.717916,
     "end_time": "2019-05-17T16:12:23.987658",
     "exception": false,
     "start_time": "2019-05-17T16:09:59.269742",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parallel: 27 partitions with 27 cores for str_timestamp_to_date_df\n",
      "parallel: 27 partitions with 27 cores for parallel_decode_df_full\n",
      "(2225657, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>category</th>\n",
       "      <th>all_text</th>\n",
       "      <th>group</th>\n",
       "      <th>txt_orig</th>\n",
       "      <th>txt_lemmatized</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2qzibe</td>\n",
       "      <td>2015-01-01 00:29:40</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>Measuring human performance on standard image ...</td>\n",
       "      <td>submissions</td>\n",
       "      <td>Measuring human performance on standard image ...</td>\n",
       "      <td>measur, human, zzzltwolfivelonelzzz, standard,...</td>\n",
       "      <td>measur, human, perform, standard, imag classif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2qzv35</td>\n",
       "      <td>2015-01-01 03:52:22</td>\n",
       "      <td>dataisbeautiful</td>\n",
       "      <td>European economy guide: Taking Europe’s pulse</td>\n",
       "      <td>submissions</td>\n",
       "      <td>European economy guide: Taking Europe’s pulse</td>\n",
       "      <td>european, economi, guid, take, europ, pul</td>\n",
       "      <td>european, economi, guid, take, europ, pul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2qzzaa</td>\n",
       "      <td>2015-01-01 05:23:25</td>\n",
       "      <td>dataisbeautiful</td>\n",
       "      <td>Popularity of visual forms in DataIsBeautiful ...</td>\n",
       "      <td>submissions</td>\n",
       "      <td>Popularity of visual forms in DataIsBeautiful ...</td>\n",
       "      <td>popular, zzzlthreelzerolfourlzzz, form, datais...</td>\n",
       "      <td>popular, visual, form, dataisbeauti, base, sam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2r048c</td>\n",
       "      <td>2015-01-01 07:16:15</td>\n",
       "      <td>dataisbeautiful</td>\n",
       "      <td>JOE GROOMING - DAILY SHAMPOO ~ best shampoo fo...</td>\n",
       "      <td>submissions</td>\n",
       "      <td>JOE GROOMING - DAILY SHAMPOO ~ best shampoo fo...</td>\n",
       "      <td>joe, groom, daili, shampoo, good, shampoo, oil...</td>\n",
       "      <td>joe, groom, daili, shampoo, good, shampoo, oil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2r0e7b</td>\n",
       "      <td>2015-01-01 10:25:46</td>\n",
       "      <td>dataisbeautiful</td>\n",
       "      <td>Los Angeles Traffic Accident Rate in Rainy vs ...</td>\n",
       "      <td>submissions</td>\n",
       "      <td>Los Angeles Traffic Accident Rate in Rainy vs ...</td>\n",
       "      <td>trafficaccid, rate, raini, dri, weather</td>\n",
       "      <td>trafficaccid, rate, raini, dri, weather</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### concatenating the dataframes + decoding the keywords + saving ###\n",
    "\n",
    "\n",
    "# File loading\n",
    "\n",
    "if c.tags is not None:\n",
    "    df_tags = pd.read_csv('{}{}_0_tags.csv'.format(c.directory['save'], c.project_name))\n",
    "    c.tags_dict = dict(zip(df_tags['key'], df_tags['val']))\n",
    "\n",
    "\n",
    "df_text_concat = pd.read_csv('{}{}_3_text_lemmatized_tagged.csv'.format(c.directory['save'], c.project_name))\n",
    "df_cols = list(df_text_concat.columns)\n",
    "\n",
    "\n",
    "df_text_concat['date']=util.parallelize_df(df_text_concat['date'], str_timestamp_to_date_df, c)\n",
    "        \n",
    "# c.init_parallel(True, c)\n",
    "df_text_concat = pd.concat([\n",
    "    df_text_concat,\n",
    "#     df_text_concat['txt_lemmatized'].parallel_apply(para_decode),\n",
    "#     util.parallelize_df(df_text_concat['txt_lemmatized'], parallel_decode_df, c),\n",
    "    util.parallelize_df(df_text_concat, parallel_decode_df_full, c),\n",
    "#     util.parallelize_df_dask(df_text_concat['txt_lemmatized'], parallel_decode_df, c),    \n",
    "],\n",
    "    ignore_index=True,\n",
    "    axis=1\n",
    ")\n",
    "df_cols.extend(['keywords'])\n",
    "df_text_concat.columns = df_cols\n",
    "df_text_concat.to_csv('{}{}_4_text_lemmatized_decoded.csv'.format(c.directory['save'], c.project_name),\n",
    "                      encoding='utf-8',\n",
    "                      index=False\n",
    "                      )\n",
    "\n",
    "try:\n",
    "    os.remove('{}{}_sql_docs_decoded.sqlite'.format(c.directory['save'],c.project_name))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "util.save_df_to_sql(df_text_concat,\n",
    "                   '{}{}_sql_docs_decoded.sqlite'.format(c.directory['save'],c.project_name),\n",
    "                    'docs'\n",
    "                   )\n",
    "pprint(df_text_concat.shape)\n",
    "util.display_df(df_text_concat.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "papermill": {
   "duration": 157.309942,
   "end_time": "2019-05-17T16:12:26.452209",
   "environment_variables": {},
   "exception": null,
   "input_path": "LDA_2_Text_Decoding.ipynb",
   "output_path": "/scratch/hkarbasi/LDA/Reddit/LDA_2_Text_Decoding_0_Reddit_NODE077.ipynb",
   "parameters": {
    "config": "configs.config_Reddit",
    "mem": 650000,
    "num_cores": 27
   },
   "start_time": "2019-05-17T16:09:49.142267",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
